<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Elahi,Ehtsham</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
</head>
<body>
<div class="wrapper">
    <header>
        <h1 class="header">ehtsham elahi</h1>
        <p class="header">living in a world of probabilistic modeling</p>
        <ul>
            <li><a class="buttons github" href="https://github.com/ehtsham">GitHub Profile</a></li>
        </ul>
    </header>
    <section>
        <h3>
            <a id="about-me" class="anchor" href="#about-me" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>About Me
        </h3>

        <img src="images/pp.jpg" alt="hi" class="inline"/>

        <p>I enjoy working on probabilistic modeling techniques to solve real-world challenges. In the vast field of
            statistical machine learning, I find non-parametric bayesian modeling techniques very attractive
            because of their mathematical elegance and ability to learn complex structures hidden in data. In my free time,
            I enjoy traveling, eating good food and watching movies with my wife, Anam.</p>

        <p><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8bEdWUVVnMkgxVGs/view?usp=sharing">Resume</a></p>

        <h3>
            <a id="technical-presentations" class="anchor" href="#technical-presentations" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Technical Presentations
        </h3>

        <p>A Distributed Gibbs Sampler for LDA using Spark and GraphX at <a href="http://mlconf.com/mlconf-2015-sea/">MLCONF 2015, Seattle</a></p>

        <ul>
            <li><a href="https://youtu.be/3pEugSc1HdQ">Video</a></li>
            <li><a href="http://www.slideshare.net/ehtshamelahi/ml-conf-seattle-2015">Presentation</a></li>
        </ul>

        <h3>
            <a id="teaching-probabilistic-graphical-models" class="anchor" href="#teaching-probabilistic-graphical-models" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Teaching Probabilistic Graphical Models
        </h3>

        <p>I lead a reading group on Advanced Probabilistic Modeling internally in Netflix. Some of my notes and implementations covering Mixture Modeling, Expectation-Maximization, Gibbs Sampling and Variational Inference are linked below:</p>

        <h4>
            <a id="notes" class="anchor" href="#notes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Notes</h4>

        <ul>
            <li><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8RmswV2t4LWxCcGM/view?usp=sharing">Maximum Likelihood Estimation</a></li>
            <li><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8TTJnQlMwcjZsSW8/view?usp=sharing">Expectation-Maximization</a></li>
            <li><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8ZHA5MmRHRDFVcTg/view?usp=sharing">Bayesian Mixtures</a></li>
            <li><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8VDUtOW14WWhhMXc/view?usp=sharing">MAP estimates</a></li>
            <li><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8eGkwRElGdk82Unc/view?usp=sharing">Posterior Sampling using Gibbs Sampling</a></li>
            <li><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8Z0VOOTFGNkVaYnM/view?usp=sharing">Posterior Sampling using Collapsed Gibbs Sampling</a></li>
            <li><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8bU80cldIaVY3dUE/view?usp=sharing">Computing Posterior Distribution using Variational Inference - 1</a></li>
            <li><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8LVNhOHhIcWNCQXc/view?usp=sharing">Computing Posterior Distribution using Variational Inference - 2</a></li>
            <li><a href="https://drive.google.com/file/d/0B7i2iJCV3z-8RlVZcmNZMVQ0bm8/view?usp=sharing">Scaling Variational Inference using Stochastic Optimization</a></li>
        </ul>

        <h4>
            <a id="ipython-notebooks" class="anchor" href="#ipython-notebooks" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>IPython Notebooks</h4>

        <ul>
            <li><a href="https://github.com/ehtsham/ehtsham.github.io/blob/master/MixtureModeling.ipynb">Multinomial Mixture Model inference using EM, Collapsed Gibbs Sampling and Stochastic Variational Inference</a></li>
        </ul>

        <h3>
            <a id="select-publications" class="anchor" href="#select-publications" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Select Publications</h3>

        <ul>
            <li><a href="https://arxiv.org/abs/1612.01481">A Nonparametric Latent Factor Model For Location-Aware Video Recommendations, NIPS Workshop on Practical Bayesian Nonparametrics, 2016</a></li>

        </ul>
    </section>
    <footer>
        <p><small>Hosted on <a href="https://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
    </footer>
</div>
<!--[if !IE]><script>fixScale(document);</script><![endif]-->

</body>
</html>
